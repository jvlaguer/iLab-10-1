{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Combining RoBERTa and BLIP-2\n",
    "This notebook will handle both text classification for yes/no questions using RoBERTa, and image description generation using BLIP-2, based on the type of input query.\n",
    "\n",
    "## Data Loading and Preprocessing:\n",
    "- Load and preprocess the dataset containing medical images (same as in Notebook 1 and 2).\n",
    "- Load the dataset of yes/no questions paired with images.\n",
    "## Image Feature Extraction and Text Generation (BLIP-2):\n",
    "- Use the pre-trained BLIP-2 model to generate image descriptions.\n",
    "## Question Classification (RoBERTa):\n",
    "- Use the RoBERTa model for binary classification (yes/no) for questions related to the medical images.\n",
    "## Switching Between Models:\n",
    "Based on the type of query, decide whether to use the RoBERTa model (for classification) or BLIP-2 (for image description generation).\n",
    "# Testing and Evaluation:\n",
    "Test the combined approach by feeding different queries and evaluating the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, RobertaTokenizer, TFRobertaForSequenceClassification\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "#Part 1: Load Models\n",
    "\n",
    "# Load BLIP-2 model and processor for image captioning\n",
    "blip2_model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\")\n",
    "blip2_processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\")\n",
    "\n",
    "# Load RoBERTa model and tokenizer for yes/no classification\n",
    "roberta_model = TFRobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Define device for RoBERTa (if using PyTorch)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "roberta_model = roberta_model.to(device)\n",
    "\n",
    "# Part 2: Image Preprocessing\n",
    "\n",
    "# Preprocessing for images (for BLIP-2)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def load_images_from_folder(folder_path):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            img = Image.open(os.path.join(folder_path, filename)).convert(\"RGB\")\n",
    "            img = preprocess(img)  # Preprocess the image\n",
    "            images.append((filename, img))\n",
    "    return images\n",
    "\n",
    "# Part 3: Text Generation (BLIP-2)\n",
    "\n",
    "def generate_image_description(image_tensor):\n",
    "    pil_image = transforms.ToPILImage()(image_tensor)\n",
    "    inputs = blip2_processor(images=pil_image, return_tensors=\"pt\")\n",
    "    outputs = blip2_model.generate(**inputs)\n",
    "    description = blip2_processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    return description\n",
    "\n",
    "# Part 4: Question Classification (RoBERTa)\n",
    "\n",
    "def classify_question(question):\n",
    "    inputs = roberta_tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    outputs = roberta_model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    return 'yes' if prediction == 1 else 'no'\n",
    "\n",
    "# Part 5: Query Handling and Evaluation \n",
    "\n",
    "def handle_query(image_path, query):\n",
    "    \"\"\"\n",
    "    Switch between image description generation and yes/no classification based on the type of query.\n",
    "    \"\"\"\n",
    "    if query.lower().startswith(\"describe\"):  # Assuming queries asking for descriptions start with \"describe\"\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image_tensor = preprocess(image)\n",
    "        return generate_image_description(image_tensor)\n",
    "    else:\n",
    "        return classify_question(query)\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate_image_and_queries(image_folder, queries):\n",
    "    images = load_images_from_folder(image_folder)\n",
    "    \n",
    "    for image_name, image_tensor in images:\n",
    "        print(f\"Processing image: {image_name}\")\n",
    "        for query in queries:\n",
    "            response = handle_query(image_name, query)\n",
    "            print(f\"Query: {query} -> Response: {response}\")\n",
    "\n",
    "# Part 6: Example Usage\n",
    "\n",
    "# Example image folder and queries\n",
    "image_folder = 'path_to_your_images'\n",
    "queries = [\n",
    "    \"Describe this medical image.\",\n",
    "    \"Is there a fracture in the image?\",\n",
    "    \"Describe the abnormalities in this radiology scan.\"\n",
    "]\n",
    "\n",
    "# Evaluate and print results\n",
    "evaluate_image_and_queries(image_folder, queries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
