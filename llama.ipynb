{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "from datasets import load_dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ROCO-radiology dataset\n",
    "ds = load_dataset(\"mdwiratathya/ROCO-radiology\")\n",
    "\n",
    "# Randomly sample 15% of the dataset\n",
    "sampled_size = int(len(ds['train']) * 0.15)\n",
    "sample_indices = random.sample(range(len(ds['train'])), sampled_size)\n",
    "sampled_data = [ds['train'][i] for i in sample_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLaMA model and processor\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision\"\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,  # Adjust dtype as per your GPU capability\n",
    "    device_map=\"auto\",  # Automatically choose the device\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img):\n",
    "    # Convert to PIL Image if not already\n",
    "    if not isinstance(img, Image.Image):\n",
    "        img = Image.fromarray(img)\n",
    "    \n",
    "    # Check the size of the image\n",
    "    width, height = img.size\n",
    "    print(f\"Original image size: {width}x{height}\")\n",
    "\n",
    "    # Convert the image to RGB if necessary\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "\n",
    "    # Resize image to (224, 224) for model input\n",
    "    img = img.resize((224, 224))\n",
    "    return img\n",
    "\n",
    "# Loop through the sampled data and generate descriptions\n",
    "for sample in sampled_data:\n",
    "    img = sample['image']  # Get the image\n",
    "    caption = sample['caption']  # Get the caption\n",
    "\n",
    "    # Preprocess the image\n",
    "    img_preprocessed = preprocess_image(img)\n",
    "\n",
    "    # Prepare prompt\n",
    "    prompt = f\"<|image|><|begin_of_text|>{caption}\"\n",
    "\n",
    "    # Process image and prompt for model input\n",
    "    inputs = processor(images=img_preprocessed, text=prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate output from the model\n",
    "    output = model.generate(**inputs, max_new_tokens=30)\n",
    "    \n",
    "    # Decode the output\n",
    "    description = processor.decode(output[0])\n",
    "    \n",
    "    print(f\"Generated Description: {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.save_pretrained(\"./llama_trained_model\")\n",
    "\n",
    "# Save the processor\n",
    "processor.save_pretrained(\"./llama_trained_processor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model summary\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to CPU before saving\n",
    "model = model.to('cpu')\n",
    "# Save the trained model\n",
    "model.save_pretrained(\"./llama_trained_model\")\n",
    "# Save the processor\n",
    "processor.save_pretrained(\"./llama_trained_processor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model with reduced shard size\n",
    "model.save_pretrained(\"./llama_trained_model\", max_shard_size=\"1GB\")\n",
    "# Save the processor as usual\n",
    "processor.save_pretrained(\"./llama_trained_processor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "\n",
    "# Load the processor\n",
    "processor = AutoProcessor.from_pretrained(\"./llama_trained_processor\")\n",
    "\n",
    "# Load the model\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\"./llama_trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "uploaded = files.upload()\n",
    "image = Image.open(io.BytesIO(uploaded[list(uploaded.keys())[0]]))\n",
    "# preprocess image\n",
    "img_preprocessed = preprocess_image(image)\n",
    "\n",
    "prompt = \"<|image|><|begin_of_text|>\"\n",
    "\n",
    "# Process image and prompt for model input\n",
    "inputs = processor(images=img_preprocessed, text=prompt, return_tensors=\"pt\").to(model.device)\n",
    "# Generate output from the model\n",
    "output = model.generate(**inputs, max_new_tokens=30)\n",
    "# Decode the output\n",
    "description = processor.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.title(f\"Generated Description: {description}\")\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
