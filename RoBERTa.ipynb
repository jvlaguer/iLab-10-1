{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import RobertaTokenizer, TFRobertaModel\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2  # Import l2 regularizer\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"flaviagiammarino/vqa-rad\")\n",
    "\n",
    "# Initialize ResNet50 model for image feature extraction\n",
    "resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze all layers except the last 10\n",
    "for layer in resnet_model.layers[:-10]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Initialize RoBERTa-large tokenizer and model\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "roberta_model = TFRobertaModel.from_pretrained('roberta-large')\n",
    "\n",
    "# Function to build vocabulary from the training set questions\n",
    "def build_vocabulary(dataset_split):\n",
    "    vocab = Counter()\n",
    "    for sample in dataset_split:\n",
    "        question = sample['question']\n",
    "        tokens = roberta_tokenizer.tokenize(question)\n",
    "        vocab.update(tokens)\n",
    "    return vocab\n",
    "\n",
    "# Build vocabulary using the training set\n",
    "vocab = build_vocabulary(dataset['train'])\n",
    "print(f\"Vocabulary Size: {len(vocab)}\")\n",
    "\n",
    "# Image preprocessing and augmentation\n",
    "def preprocess_image(img):\n",
    "    img = tf.image.resize(img, (224, 224))\n",
    "    img = tf.image.random_flip_left_right(img)  # Augmentation: Flip\n",
    "    img = tf.image.random_brightness(img, 0.2)  # Augmentation: Brightness adjustment\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "# Feature extraction: image (ResNet50) and text (RoBERTa)\n",
    "def process_image_text(sample):\n",
    "    img = sample['image']\n",
    "    img_array = preprocess_image(img)\n",
    "    \n",
    "    # Extract image features using ResNet50\n",
    "    img_features = resnet_model.predict(img_array)\n",
    "    img_features = img_features.flatten()  # Flatten the image features\n",
    "    \n",
    "    # Tokenize and encode the question using RoBERTa\n",
    "    question = sample['question']\n",
    "    inputs = roberta_tokenizer.encode_plus(question, max_length=512, return_attention_mask=True, return_tensors='tf')\n",
    "    outputs = roberta_model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "    text_embeddings = outputs.last_hidden_state[:, 0, :].numpy().flatten()  # Extract the CLS token and flatten\n",
    "    \n",
    "    # Fuse the image features and text embeddings\n",
    "    combined_features = np.concatenate([img_features, text_embeddings])\n",
    "    return combined_features\n",
    "\n",
    "# Process dataset and convert to features\n",
    "def extract_features(dataset_split):\n",
    "    features = []\n",
    "    for sample in dataset_split:\n",
    "        features.append(process_image_text(sample))\n",
    "    return np.array(features)\n",
    "\n",
    "# Extract features for train and test datasets\n",
    "train_features = extract_features(dataset['train'])\n",
    "test_features = extract_features(dataset['test'])\n",
    "\n",
    "# Convert labels (\"yes\"/\"no\" to binary)\n",
    "train_labels = np.array([1 if answer == 'yes' else 0 for answer in dataset['train']['answer']])\n",
    "test_labels = np.array([1 if answer == 'yes' else 0 for answer in dataset['test']['answer']])\n",
    "train_labels_cat = to_categorical(train_labels, num_classes=2)\n",
    "test_labels_cat = to_categorical(test_labels, num_classes=2)\n",
    "\n",
    "# Define the VQA classification model using Functional API\n",
    "input_layer = Input(shape=(train_features.shape[1],))  # Adjust the input shape to match the feature vector\n",
    "x = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(input_layer)  # Use l2 regularizer\n",
    "x = Dropout(0.5)(x)  # Dropout for regularization\n",
    "output_layer = Dense(2, activation='softmax')(x)  # Binary output (yes/no)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model with 20 epochs\n",
    "model.fit(train_features, train_labels_cat, epochs=20, batch_size=32, validation_data=(test_features, test_labels_cat))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(test_features, test_labels_cat)\n",
    "print(f'Test Loss: {loss:.3f}')\n",
    "print(f'Test Accuracy: {accuracy:.3f}')\n",
    "\n",
    "# Predict and display classification report\n",
    "predictions = model.predict(test_features)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print('Classification Report:')\n",
    "print(classification_report(test_labels, predicted_classes))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(test_labels, predicted_classes))\n",
    "\n",
    "# Save the model as a Keras model\n",
    "model.save('vqa_model_roberta.keras')\n",
    "\n",
    "print(f'Test Loss: {loss:.3f}')\n",
    "print(f'Test Accuracy: {accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('vqa_model_roberta.h5')\n",
    "model.save('vqa_model_roberta.keras')\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "model =load_model('vqa_model_roberta.keras')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import RobertaTokenizer, TFRobertaModel\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"flaviagiammarino/vqa-rad\")\n",
    "\n",
    "# Initialize RoBERTa-large tokenizer and model\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "roberta_model = TFRobertaModel.from_pretrained('roberta-large')\n",
    "\n",
    "# Unfreeze all layers in RoBERTa for fine-tuning\n",
    "roberta_model.trainable = True\n",
    "\n",
    "# Initialize ResNet50 model for image feature extraction\n",
    "resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Unfreeze all layers in ResNet50 for fine-tuning\n",
    "resnet_model.trainable = True\n",
    "\n",
    "# Function to preprocess and augment the image\n",
    "def preprocess_image(img):\n",
    "    img = tf.image.resize(img, (224, 224))\n",
    "    img = tf.image.random_flip_left_right(img)  # Augmentation: Flip\n",
    "    img = tf.image.random_brightness(img, 0.2)  # Augmentation: Brightness adjustment\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "# Feature extraction: image (ResNet50) and text (RoBERTa)\n",
    "def process_image_text(sample):\n",
    "    img = sample['image']\n",
    "    img_array = preprocess_image(img)\n",
    "    \n",
    "    # Extract image features using ResNet50\n",
    "    img_features = resnet_model.predict(img_array)\n",
    "    img_features = img_features.flatten()  # Flatten the image features\n",
    "    \n",
    "    # Tokenize and encode the question using RoBERTa\n",
    "    question = sample['question']\n",
    "    inputs = roberta_tokenizer.encode_plus(question, max_length=512, return_attention_mask=True, return_tensors='tf')\n",
    "    outputs = roberta_model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "    text_embeddings = outputs.last_hidden_state[:, 0, :].numpy().flatten()  # Extract the CLS token and flatten\n",
    "    \n",
    "    # Fuse the image features and text embeddings\n",
    "    combined_features = np.concatenate([img_features, text_embeddings])\n",
    "    return combined_features\n",
    "\n",
    "# Process dataset and convert to features\n",
    "def extract_features(dataset_split):\n",
    "    features = []\n",
    "    for sample in dataset_split:\n",
    "        features.append(process_image_text(sample))\n",
    "    return np.array(features)\n",
    "\n",
    "# Extract features for train and test datasets\n",
    "train_features = extract_features(dataset['train'])\n",
    "test_features = extract_features(dataset['test'])\n",
    "\n",
    "# Convert labels (\"yes\"/\"no\" to binary)\n",
    "train_labels = np.array([1 if answer == 'yes' else 0 for answer in dataset['train']['answer']])\n",
    "test_labels = np.array([1 if answer == 'yes' else 0 for answer in dataset['test']['answer']])\n",
    "train_labels_cat = to_categorical(train_labels, num_classes=2)\n",
    "test_labels_cat = to_categorical(test_labels, num_classes=2)\n",
    "\n",
    "# Define the VQA classification model using Functional API\n",
    "input_layer = Input(shape=(train_features.shape[1],))  # Adjust the input shape to match the feature vector\n",
    "x = Dense(1024, activation='relu', kernel_regularizer=l2(0.01))(input_layer)\n",
    "x = BatchNormalization()(x)  # Batch normalization for regularization\n",
    "x = Dropout(0.5)(x)  # Dropout for regularization\n",
    "x = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "x = Dropout(0.5)(x)  # Dropout for regularization\n",
    "output_layer = Dense(2, activation='softmax')(x)  # Binary output (yes/no)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Learning rate scheduler\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return float(lr * tf.math.exp(-0.1))  # Convert tensor to float\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "\n",
    "# Train the model with 20 epochs and include learning rate scheduler\n",
    "history = model.fit(train_features, train_labels_cat, epochs=20, batch_size=32, \n",
    "                    validation_data=(test_features, test_labels_cat), \n",
    "                    callbacks=[lr_scheduler])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(test_features, test_labels_cat)\n",
    "print(f'Test Loss: {loss:.3f}')\n",
    "print(f'Test Accuracy: {accuracy:.3f}')\n",
    "\n",
    "# Plot learning curve\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save the fine-tuned model as a Keras model\n",
    "model.save('vqa_model_roberta_finetuned.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the VQA classification model using Functional API\n",
    "input_layer = Input(shape=(train_features.shape[1],))\n",
    "x = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)  # Dropout for regularization\n",
    "x = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)  # Dropout for regularization\n",
    "output_layer = Dense(2, activation='softmax')(x)  # Binary output (yes/no)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model with a smaller learning rate\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model with 20 epochs\n",
    "history = model.fit(train_features, train_labels_cat, epochs=20, batch_size=32, \n",
    "                    validation_data=(test_features, test_labels_cat), \n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(test_features, test_labels_cat)\n",
    "print(f'Test Loss: {loss:.3f}')\n",
    "print(f'Test Accuracy: {accuracy:.3f}')\n",
    "\n",
    "# Plot the learning curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Learning Curve - Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Learning Curve - Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
