{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/hemang/.cache/huggingface/datasets/flaviagiammarino___parquet/flaviagiammarino--vqa-rad-d04980c9c3579419/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6705b664224fe8a604b47653413576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the VQA-RAD dataset\n",
    "dataset = load_dataset(\"flaviagiammarino/vqa-rad\")\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['test']  # Assuming there is a validation split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hemang/anaconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/hemang/anaconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Check if MPS (Metal Performance Shaders) is available and set the device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Load and configure ResNet-50 for image feature extraction\n",
    "resnet50 = models.resnet50(pretrained=True).to(device)\n",
    "resnet50.eval()\n",
    "\n",
    "# Define image transformations\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def extract_image_features(image):\n",
    "    image = image_transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = resnet50(image)\n",
    "    return features.squeeze().cpu()\n",
    "\n",
    "# Load and configure RoBERTa for question feature extraction\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
    "roberta = AutoModel.from_pretrained(\"roberta-large\").to(device)\n",
    "roberta.eval()\n",
    "\n",
    "def extract_question_features(question):\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\", truncation=True, padding=True, max_length=128).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = roberta(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Applying Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b402b0e28f2475ebb2b7c7ee88be2bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1793 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hemang/anaconda3/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808abbb9609e42718fd18a4f91af766f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/451 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_and_store_features(dataset):\n",
    "    def extract_features(example):\n",
    "        image_features = extract_image_features(example['image'])\n",
    "        question_features = extract_question_features(example['question'])\n",
    "        return {\n",
    "            'image_features': image_features.tolist(),\n",
    "            'question_features': question_features.tolist(),\n",
    "            'label': example['answer']  # Assuming the label is in the 'answer' field\n",
    "        }\n",
    "    \n",
    "    # Apply the feature extraction\n",
    "    dataset = dataset.map(extract_features, batched=False)\n",
    "    return dataset\n",
    "\n",
    "# Apply extraction to both the train and validation datasets\n",
    "train_dataset = extract_and_store_features(train_dataset)\n",
    "val_dataset = extract_and_store_features(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Feature Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23823ddbeab1400cb57101a5df9d01f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1793 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "604a892dd53d4be496638819614f3696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/451 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fuse_features(example):\n",
    "    fused_features = torch.cat((torch.tensor(example['image_features']), torch.tensor(example['question_features'])), dim=0)\n",
    "    return {'fused_features': fused_features.tolist(), 'label': example['label']}\n",
    "\n",
    "# Apply feature fusion to the train and validation datasets\n",
    "train_dataset = train_dataset.map(fuse_features, batched=False)\n",
    "val_dataset = val_dataset.map(fuse_features, batched=False)\n",
    "\n",
    "# Optionally, remove the individual feature columns\n",
    "train_dataset = train_dataset.remove_columns(['image_features', 'question_features'])\n",
    "val_dataset = val_dataset.remove_columns(['image_features', 'question_features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 b Prepare DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define the Model Architecture\n",
    "\n",
    "- Image Feature Input: Use the extracted features from ResNet-50.\n",
    "- Question Feature Input: Use the embeddings from RoBERTa or another text processor.\n",
    "- Fusion Layer: Combine image and question features.\n",
    "- LSTM Layer: Pass the fused features through an LSTM layer for sequence modeling.\n",
    "- Output Layer: Use a dense layer to map the LSTM outputs to the answer classes (classification) or to generate the final answer.\n",
    "2. Loss Function and Optimizer\n",
    "- Loss Function: For classification, use Cross-Entropy Loss since the answers are categorical.\n",
    "- Optimizer: Use Adam optimizer, which is well-suited for training deep learning models due to its adaptive learning rate.\n",
    "- Learning Rate: Set an initial learning rate, e.g., 1e-4, and consider using a learning rate scheduler to reduce the rate as training progresses.\n",
    "3. Training the Model\n",
    "- Batch Size: Select an appropriate batch size, such as 32 or 64, depending on your system's memory.\n",
    "- Epochs: Start with 10-20 epochs and adjust based on the model's performance.\n",
    "- Data Augmentation: If needed, apply techniques like random cropping or flipping to the image data to increase diversity.\n",
    "- Validation Split: Use a portion of the training data for validation (e.g., 10-20%) to monitor the model's performance during training.\n",
    "4. Evaluation Metrics\n",
    "- Accuracy: Calculate the percentage of correct answers.\n",
    "- Precision, Recall, and F1 Score: These metrics are especially important for understanding the model's performance on imbalanced datasets.\n",
    "- Confusion Matrix: Provides insights into specific types of errors the model is making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Prepare DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'not seen here'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Prepare DataLoaders for training and validation datasets\u001b[39;00m\n\u001b[1;32m     23\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m prepare_dataloader(train_dataset)\n\u001b[0;32m---> 24\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 9\u001b[0m, in \u001b[0;36mprepare_dataloader\u001b[0;34m(dataset, batch_size, shuffle)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_dataloader\u001b[39m(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Convert labels from strings to integers using the mapping\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([label_to_index[label] \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Convert fused features to a tensor\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     fused_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused_features\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[26], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_dataloader\u001b[39m(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Convert labels from strings to integers using the mapping\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[43mlabel_to_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Convert fused features to a tensor\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     fused_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused_features\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'not seen here'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Step 1: Create a label mapping (example mapping)\n",
    "label_to_index = {label: idx for idx, label in enumerate(set(train_dataset['label']))}\n",
    "\n",
    "# Step 2: Update DataLoader preparation function\n",
    "def prepare_dataloader(dataset, batch_size=32, shuffle=True):\n",
    "    # Convert labels from strings to integers using the mapping\n",
    "    labels = torch.tensor([label_to_index[label] for label in dataset['label']])\n",
    "    \n",
    "    # Convert fused features to a tensor\n",
    "    fused_features = torch.tensor(dataset['fused_features'])\n",
    "    \n",
    "    # Create a TensorDataset\n",
    "    tensor_dataset = TensorDataset(fused_features, labels)\n",
    "    \n",
    "    # Create a DataLoader\n",
    "    dataloader = DataLoader(tensor_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "# Prepare DataLoaders for training and validation datasets\n",
    "train_loader = prepare_dataloader(train_dataset)\n",
    "val_loader = prepare_dataloader(val_dataset, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Define the Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VQAModel(nn.Module):\n",
    "    def __init__(self, image_feature_dim, question_feature_dim, hidden_dim, output_dim):\n",
    "        super(VQAModel, self).__init__()\n",
    "        \n",
    "        # Image feature processing\n",
    "        self.image_fc = nn.Linear(image_feature_dim, hidden_dim)\n",
    "        \n",
    "        # Question feature processing\n",
    "        self.question_fc = nn.Linear(question_feature_dim, hidden_dim)\n",
    "        \n",
    "        # LSTM layer for sequence modeling\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Final output layer\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, image_features, question_features):\n",
    "        # Process image features\n",
    "        image_features = F.relu(self.image_fc(image_features))\n",
    "        \n",
    "        # Process question features\n",
    "        question_features = F.relu(self.question_fc(question_features))\n",
    "        \n",
    "        # Concatenate image and question features\n",
    "        combined_features = torch.cat((image_features.unsqueeze(1), question_features.unsqueeze(1)), dim=1)\n",
    "        \n",
    "        # Sequence modeling with LSTM\n",
    "        lstm_out, _ = self.lstm(combined_features)\n",
    "        \n",
    "        # Use the output from the last LSTM cell\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Apply dropout\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        \n",
    "        # Final output layer\n",
    "        output = self.fc_out(lstm_out)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Parameters for the model\n",
    "image_feature_dim = 2048  # Example: features from ResNet-50\n",
    "question_feature_dim = 768  # Example: embeddings from RoBERTa\n",
    "hidden_dim = 512  # Dimension of the hidden layer in LSTM\n",
    "output_dim = 100  # Number of possible answers (adjust based on your dataset)\n",
    "\n",
    "# Initialize the model\n",
    "model = VQAModel(image_feature_dim, question_feature_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Training and Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fused_features, labels \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_loader\u001b[49m:\n\u001b[1;32m     11\u001b[0m     fused_features, labels \u001b[38;5;241m=\u001b[39m fused_features\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for fused_features, labels in train_loader:\n",
    "        fused_features, labels = fused_features.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(fused_features[:, :image_feature_dim], fused_features[:, image_feature_dim:])\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    train_accuracy = 100. * correct / total\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss/len(train_loader):.4f}, Accuracy: {train_accuracy:.2f}%')\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for fused_features, labels in val_loader:\n",
    "            fused_features, labels = fused_features.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(fused_features[:, :image_feature_dim], fused_features[:, image_feature_dim:])\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    val_accuracy = 100. * val_correct / val_total\n",
    "    print(f'Validation Loss: {val_loss/len(val_loader):.4f}, Validation Accuracy: {val_accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
